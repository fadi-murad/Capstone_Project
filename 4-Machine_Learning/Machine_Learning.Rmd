---
title: "Machine Learning"
author: "Fadi Murad"
date: "July 1, 2018"
output:
    html_document:
      theme: cerulean
      toc: true
      toc_depth: 4
      toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE,comment="")
```


### 1. Ideas to Start
Before we start with modeling let us take a minute to clarify a few things about this step and imbalanced dataset issue  

####1.1. Best Performance Measure
***Is accuracy measure the best measure for imbalanced dataset classification model? Well! it's not***  

It's very common for classification model (especially with simple rule-based algorithm) to give ~90% accuracy when we have 90% of the instances fall into one class, simply because our models look at the data and cleverly decide that the best thing to do is to always predict this majority class and achieve high accuracy

*Solution!*  

Below performance measures can give more insight into the accuracy of our model

1. Sensitivity = True Positive Rate (TP/TP+FN) 
2. Specificity = True Negative Rate (TN/TN +FP)
3. Precision = (TP/TP+FP)
4. Recall = Sensitivity
5. ROC Curves and Area Under Curve (AUC)

In this project, we will use *Area Under the Curve (AUC)* to be the main performance measure for the below reasons:

* Usually AUC is a very good measure to use with imbalanced data.
* We are trying to build a model that has a good prediction for both classes (0 and 1 or <50,000 and > 50,000). in other words, we don't have a preference or we don't value one class accuracy over the other and AUC can do that, unlike Sensitivity for example which focus on increasing the TP rate  


####1.2. Sampling Techniques
***Do we need sampling techniques?***  

For imbalanced data, it's recommended to change the dataset to be more balanced in order to use it to build our predictive model  

This change is called sampling dataset and there are a few techniques could be used, in this project, we will use over-sampling & under-sampling techniques  

We will create 2 additional sets using the mentioned methods then we will apply the classification models on the original and the new sets and evaluate the results  


#### 1.3. Algorithms
We will use the below algorithms in this project:

1. Logistic Regression: we will start with a simple classification model 
2. Random Forest: often perform well on imbalanced data 
3. naive Bayes: also a good algorithm for the imbalanced data  

We have 2 very good packages for predictive modeling CARET and MLR, however, due to the limited performance of my machine I will use MLR (train function in CARET package consume a high portion of memory during the runtime and create huge size objects)

***Now after we have a clear vision let us hit the road!***  

<br />  

### 2. Machine Learning

We will create tasks for train, test, over sampling, under sampling and set our validation strategy

```{r}
library(rJava)
library(mlr)
library(lattice)
library(ggplot2)
library(FSelector)
library(ROCR)

set.seed(50)

load(file="f_tr.RData")
load(file="f_test.RData")

#create task for train and test sets
tr_Task <- makeClassifTask(data = f_tr,target = "income_level")
test_Task <- makeClassifTask(data = f_test,target = "income_level")

#Create sampling tasks in order to make the data balanced
#under-sampling 
tr_Under <- undersample(tr_Task,rate = 0.1) #keep only 10% of majority class

#over-sampling
tr_Over <- oversample(tr_Task,rate=15) #multiply minority class 15 times

#Set Validation Strategy: 5 fold cross validation
res_Mod <- makeResampleDesc("CV",iters=5 ,stratify = TRUE)

```

<br />  

Let us take a look at how important each variable to the target variable, we will use **generateFilterValuesData** function  


```{r}
variable_Imp <- generateFilterValuesData(tr_Task,method = "information.gain")
plotFilterValues(variable_Imp)

```

Education and major occupation code could provide the highest info for our model while race variable has very litter to offer here  

<br />  

#### 2.1 Logistic Regression Model

*Set learner for logistic regression*
```{r}
logR_learner <- makeLearner("classif.logreg",predict.type = "prob", fix.factors.prediction = TRUE)

```
Some predictive models return an error when we try to predict new dataset (test set in our case) if the new set does not have all levels of a factor.
Logistic Regression and Naive Bayes have this issue, in order to avoid this, we can add argument *fix.factors.prediction = TRUE* to our learner  

*Cross-validation for the 3 sets (original, under, over)*

```{r cache= TRUE }

logRCV_Task<- resample(logR_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
logRCV_Under<- resample(logR_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
logRCV_Over<- resample(logR_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
```

After running the resample function we got the below warning messages:  

*1: glm.fit: fitted probabilities numerically 0 or 1 occurred*  

*2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :prediction from a rank-deficient fit may be misleading*  

The 1st warning could be due to separation. 
There are many suggested solutions for this issue, do nothing and ignore the warning, drop the variables that cause this problem but in this case, we will lose information, and finally, utilize a form of penalized regression, in fact, this is the original reason some of the penalized regression forms were developed  

The 2nd warning could be due to excess of variables (this often happen in logistic regression when we have too many variables)

Anyhow, we will not go further in the logistic regression model, I just wanted to show the limitation of this model for our dataset. Let us check naive bayes model, we may have better luck with it.

<br />  



#### 2.2 Naive Bayes

*Set learner for naive bayes*
```{r}
naiveBa_learner <- makeLearner("classif.naiveBayes",predict.type = "prob", fix.factors.prediction = TRUE)
```

*Cross-validation for the 3 sets (original, under, over)*

```{r cache= TRUE, results='hide'}
naiveBaCV_Task<- resample(naiveBa_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
naiveBaCV_Under<- resample(naiveBa_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
naiveBaCV_Over<- resample(naiveBa_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))

```
We applied 5 fold cross-validation on our resampling, therefore, before checking the aggregate result, we will take a look at the performance of each iteration in each set to see if there is abnormal behavior between the iterations  

There is more than one way to plot ROC curves, in this project we will use **generateThreshVsPerfData** and **plotROCCurves** , while we will use ggplot function to plot the error "mmce" 

*error and auc in each iter in each set*
```{r fig.height= 5 , fig.width=12}

gridExtra::grid.arrange(
  ggplot(naiveBaCV_Task$measures.test,aes(iter,y = value))+geom_point(aes(y=mmce),col = "red")+geom_point(aes(y=naiveBaCV_Under$measures.test$mmce),col = "blue")+geom_point(aes(y=naiveBaCV_Over$measures.test$mmce),col = "green")+ggtitle("mmce"),
  plotROCCurves(generateThreshVsPerfData(list(Original = naiveBaCV_Task,Over = naiveBaCV_Over, Under = naiveBaCV_Under),measures = list(fpr,tpr,mmce), aggregate = FALSE ))+ggtitle("ROC"),
  ncol = 2
)
  
```

Well! it seems all the iterations have very similer measures in each task  


*save performance measures, remove resample objects to save memory, and print performance measures*

```{r}
nb_Original<- naiveBaCV_Task$aggr
nb_Under<- naiveBaCV_Under$aggr
nb_Over<- naiveBaCV_Over$aggr

remove(naiveBaCV_Task,naiveBaCV_Over,naiveBaCV_Under)

cbind(as.data.frame(nb_Original),as.data.frame(nb_Under),as.data.frame(nb_Over))
```

All 3 models almost have the same AUC *0.92* let us train all of them and check their performance then  

<br />  

*Train & Prediction*
```{r cache= TRUE,results='hide'}
naiveBa_Mod_Original <- train(naiveBa_learner, tr_Task)
naiveBa_Pre_Original <- predict(naiveBa_Mod_Original,test_Task)

naiveBa_Mod_Over <- train(naiveBa_learner, tr_Over)
naiveBa_Pre_Over <- predict(naiveBa_Mod_Over,test_Task)

naiveBa_Mod_Under <- train(naiveBa_learner, tr_Under)
naiveBa_Pre_Under <- predict(naiveBa_Mod_Under,test_Task)

```



Let us visualize the result. Usually with mlr package we can use *getConfMatrix* to show confusion matrix but in order to make a plot I used *fourfoldplot* and *table* functions.  

*confusion matrix*
<br />  

```{r}
par(mfrow=c(1,3)) 
fourfoldplot(table(Actual = f_test$income_level,Predicted = naiveBa_Pre_Original$data$response))
title("Original")
fourfoldplot(table(Actual = f_test$income_level,Predicted = naiveBa_Pre_Over$data$response))
title("Over")
fourfoldplot(table(Actual = f_test$income_level,Predicted = naiveBa_Pre_Under$data$response))
title("Under")

```


*ROC curve(fpr vs. tpr) and Precision vs. Recall*

```{r fig.height= 5 , fig.width=12}
naiveBaROCR_Th <- generateThreshVsPerfData(list(Original =naiveBa_Pre_Original, Over = naiveBa_Pre_Over, Under = naiveBa_Pre_Under),
                                         measures = list(tpr,fpr,ppv) )
gridExtra::grid.arrange(plotROCCurves(naiveBaROCR_Th,measures = list(fpr,tpr),diagonal = FALSE)+ggtitle("AUC") ,
                        plotROCCurves(naiveBaROCR_Th,measures = list(tpr,ppv),diagonal = FALSE) + ggtitle("Precision vs Recall"),ncol = 2)

```

<br />  

It seems original model is slightly better than other models (especially with tpr measure). We will consider the original model as the best one on the naive bayes  


```{r}
mlr::performance(naiveBa_Pre_Original,list(auc))

```
However, let us see if we can get better result with random forest model

<br />  


#### 2.3 Random Forest Model

*Set learner for random forest*
```{r}
randF_learner <- makeLearner("classif.h2o.randomForest",predict.type = "prob")
```

*Cross-validation for the 3 sets (original, under, over)*
```{r cache= TRUE, results='hide'}
randFCV_Task<- resample(randF_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
randFCV_Under<- resample(randF_learner,tr_Under,res_Mod,
                         measures = list(auc,acc,tpr,tnr,fpr,mmce))
randFCV_Over<- resample(randF_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))

```


*error and auc in each iter in each set*
```{r fig.height= 5 , fig.width=12}

gridExtra::grid.arrange(
  ggplot(randFCV_Task$measures.test,aes(iter,y = value))+geom_point(aes(y=mmce),col = "red")+geom_point(aes(y=randFCV_Under$measures.test$mmce),col = "blue")+geom_point(aes(y=randFCV_Over$measures.test$mmce),col = "green")+ggtitle("mmce"),
  plotROCCurves(generateThreshVsPerfData(list(Original = randFCV_Task,Over = randFCV_Over, Under = randFCV_Under),measures = list(fpr,tpr,mmce), aggregate = FALSE ))+ggtitle("AUC"),
  ncol = 2
)
  
```

Again as in naive bayes, all the iterations have very similer measures in each task  

*Save performance measures, remove resample objects to save memory and Show performance measures in one table*
```{r}
AggrandFCV_Task<-randFCV_Task$aggr
AggrandFCV_Under<-randFCV_Under$aggr
AggrandFCV_Over<-randFCV_Over$aggr

remove(randFCV_Over,randFCV_Task,randFCV_Under)

cbind(as.data.frame(AggrandFCV_Task),as.data.frame(AggrandFCV_Under),as.data.frame(AggrandFCV_Over))

```


Original task perform bad for minority class, over-sampling has very high auc 0.99 while under-sampling has a good auc with 0.94
I am gonna train both under and over tasks to compare their performance on the test set as I suspect overfitting for over-sampling task

<br />  

*Train & Prediction*

```{r cache= TRUE,results='hide'}

randF_Mod_Over <- train(randF_learner, tr_Over)
randF_Pre_Over <- predict(randF_Mod_Over,test_Task)

randF_Mod_Under <- train(randF_learner, tr_Under)
randF_Pre_Under <- predict(randF_Mod_Under,test_Task)
```

Let us visualize the result  

*confusion matrix*

```{r}
par(mfrow=c(1,2)) 
fourfoldplot(table(Actual = f_test$income_level,Predicted = randF_Pre_Over$data$response))
title("Over")
fourfoldplot(table(Actual = f_test$income_level,Predicted = randF_Pre_Under$data$response))
title("Under")
```

<br />  

*ROC curve(fpr vs. tpr) and Precision vs. Recall*

```{r fig.height= 5 , fig.width=12}
randFROCR_Th <- generateThreshVsPerfData(list(Over = randF_Pre_Over, Under = randF_Pre_Under), measures = list(tpr,fpr,ppv) )

gridExtra::grid.arrange(plotROCCurves(randFROCR_Th,measures = list(fpr,tpr),diagonal = FALSE)+ggtitle("AUC") , plotROCCurves(randFROCR_Th,measures = list(tpr,ppv),diagonal = FALSE) + ggtitle("Precision vs Recall"),ncol = 2)

```

<br />  

Both models have almost same performance measures in the test set, but over-sampling have 0.99 AUC on the training set and 0.94 on the test set while under-sampling maintains his AUC performance on both sets 0.94 which indicate that under-sampling is the more reliable model  

Also this model is better than the original naive bayes model(auc 0.92) therefore we can say that our best model in this project is random forest under-sampling with 0.94 auc

*Best model performance measures*
```{r}
mlr::performance(randF_Pre_Under,list(auc,acc,tpr,tnr,fpr,mmce))

```
<br />  

#### 2.4 Improvement?
Actually there are some steps that we can do to imporve our model result but due to the limitation of my machine performance I couldn't run those steps. Below you can find 2 examples to improve our random forest under-sampling model

##### 2.4.1 Feature Selection
We can use makeFeatSelControlRandom and selectFeatures functions to chose features that maximaze the auc measure as follow:

ctrl <- makeFeatSelControlRandom(maxit = 5)  

randF_SF <- selectFeatures(randF_learner,tr_Under,res_Mod,
                        measures = list(auc,acc,tpr,tnr),control = ctrl)

##### 2.4.1 Model Tuning
Also we can tune the parameters in the model using tuneParams function  

*Search for hyperparameters*  
randF_MP <- makeParamSet(makeIntegerParam("minsplit",lower = xx, upper = xx),
makeIntegerParam("minbucket", lower = xx, upper = xx),
makeNumericParam("cp", lower = xx, upper = xx))  

*do a grid search*  
randF_C <- makeTuneControlGrid()  

*hypertune the parameters*  

stune <- tuneParams(learner = randF_learner, resampling = res_Mod, task = tr_Under, par.set = randF_MP, control = randF_C, measures = auc)  




