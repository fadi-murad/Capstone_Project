---
title: "Machine Learning"
author: "Fadi Murad"
date: "July 1, 2018"
output:
    html_document:
      theme: cerulean
      toc: true
      toc_depth: 4
      toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE ,message=FALSE,comment="")
```


### 1. Ideas to Start
Before we start with modeling let us take a minute to clarify a few things about this step and imbalanced dataset issue

####1.1. Best Performance Measure
*Is accurecy measure the best measure for imbalanced dataset classification model, Well! it's not*  

It's very common for classification model (especially with simple rule based algorithm) to give ~90% accurecy when we have 90% of the instances fall into one class, simply because our models look at the data and cleverly decide that the best thing to do is to always predict this majority class and achieve high accuracy

*Solution!*  

Below performance measures can give more insight into the accuracy of our model

1. Sensitivity = True Positive Rate (TP/TP+FN) 
2. Specificity = True Negative Rate (TN/TN +FP)
3. Precision = (TP/TP+FP)
4. Recall = Sensitivity
5. F score (A weighted average of precision and recall) = () 2 * (Precision * Recall)/ (Precision + Recall) â€“ It is the harmonic mean of precision and recall. It is used to compare several models side-by-side. Higher the better.
6. ROC Curves and Area Under Curve (AUC)

In this project I chose *area under the curve (AUC)* to be the main performance measure.
a. Usually AUC is a very good measure to be used with imbalanced data
b. We are trying to build a model that have a balance between predicting the 2 classes we have (0 and 1 or <50,000 and > 50,000). in other words, we don't have a preference about the importance of one class over the other and AUC can do that, unlike Sensitivity for example which focus on increase the TP rate 

####1.2. Sampling Techniques
*Do we need to sampling techniques?*
For imbalanced data, it's recommended to change the dataset to more balanced dataset in order to use it to build our predictive model
This change is called sampling dataset and there are a few techniques, in this project we will use over-sampling & under-sampling techniques
We will create 2 addional sets using the mentioned methods then we will apply the classification models on the original and the new sets and evaluate the results

#### 1.3. Algorithms
We will use the below algorithms in this project:
1. Logistic Regression: we will start with a simple classification model 
2. Random Forest: often perform well on imbalanced data 
3. naive Bayes:also good algorithm for the imbalanced data
4. SVM

We have 2 very good packages for predictive modeling CARET and MLR, due to the limited performance of my machine I will use MLR (train function in CARET package consum high portion of memory during the run time and create huge size objects)

*Now after we have a clear vision let us hit the road!*

### 2. Machine Learning

We will create tasks for train, test, over sampling, under sampling and set our validaion strategy

```{r}
library(caret)
library(rJava)
library(mlr)
library(lattice)
library(ggplot2)
library(FSelector)
library(ROCR)

set.seed(50)

load(file="f_tr.RData")
load(file="f_test.RData")


#create task for train and test sets
tr_Task <- makeClassifTask(data = f_tr,target = "income_level")
test_Task <- makeClassifTask(data = f_test,target = "income_level")

#Sampling in order to make the data balanced
tr_Under <- undersample(tr_Task,rate = 0.1) #keep only 10% of majority class

#oversampling
tr_Over <- oversample(tr_Task,rate=15) #multiply minority class 15 times

#Validation Strategy
res_Mod <- makeResampleDesc("CV",iters=4,stratify = TRUE)

```

Let us take a look to how important each variable to the target variable, we will use generateFilterValuesData function
Education and major occupation code could provide the highest info for our model while race variable has very litter to offer here

```{r}
variable_Imp <- generateFilterValuesData(tr_Task,method = "information.gain")
plotFilterValues(variable_Imp)

```


#### 2.1 Logistic Reg Model
```{r}
#set learner for log. reg.
logR_learner <- makeLearner("classif.logreg",predict.type = "prob")



```

```{r}
#cross validation for the 3 sets (original, under, over)
logRCV_Task<- resample(logR_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,fp,fn))
logRCV_Under<- resample(logR_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,fp,fn))
logRCV_Over<- resample(logR_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,fp,fn))
```

```{r}
#save performance measures
AgglogRCV_Task<- logRCV_Task$aggr
AgglogRCV_Under<- logRCV_Under$aggr
AgglogRCV_Over<- logRCV_Over$aggr

```

```{r}
#remove resample objects to save memory
remove(logRCV_Task,logRCV_Over,logRCV_Under)
```


```{r}
#Show performance measures
AgglogRCV_Task
AgglogRCV_Task
AgglogRCV_Task

```

For the original and over sampling task we got the below warnings:
Warning messages:
1: glm.fit: fitted probabilities numerically 0 or 1 occurred 
2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :prediction from a rank-deficient fit may be misleading

All the three models have almost the same AUC but:
It seems we have overfitting case for the original task, it tends to predict the majority class very good, while it has very poor reslut for the minority
over and under sampling seem to have more balanced reslut 



#### 2.2 Random Forest Model

```{r}
randF_learner <- makeLearner("classif.h2o.randomForest",predict.type = "prob")

#cross function
randFCV_Task<- resample(randF_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,fp,fn))
randFCV_Under<- resample(randF_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,fp,fn))
randFCV_Over<- resample(randF_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,fp,fn))

#Summary of CV
AggrandFCV_Task<-randFCV_Task$aggr
AggrandFCV_Under<-randFCV_Under$aggr
AggrandFCV_Over<-randFCV_Over$aggr

remove(randFCV_Over,randFCV_Task,randFCV_Under)

AggrandFCV_Task
AggrandFCV_Under
AggrandFCV_Over

```



As in logistic reg. original task perform bad for minority class, over task has very high auc 0.99 while under task has a high auc with 0.94
I am gonna train both under and over tasks becuase i think over may be overfit model

```{r}
#Train & Prediction
randF_Mod <- train(randF_learner, tr_Over)
randF_Pre <- predict(randF_Mod,test_Task)


randF_Mod1 <- train(randF_learner, tr_Under)
randF_Pre1 <- predict(randF_Mod1,test_Task)

mlr::performance(randF_Pre,list(auc,acc,tpr,tnr,fpr,fp,fn))
mlr::performance(randF_Pre1,list(auc,acc,tpr,tnr,fpr,fp,fn))


#ROC
randFROCR_Pre <- asROCRPrediction(randF_Pre)
randFROCR_PreP <- ROCR::performance(randFROCR_Pre,"tpr","fpr")
randFROCR_Pre1 <- asROCRPrediction(randF_Pre1)
randFROCR_PreP1 <- ROCR::performance(randFROCR_Pre1,"tpr","fpr")


ROCR::plot(randFROCR_PreP,col = "red")
ROCR::plot(randFROCR_PreP1,col = "blue",add = TRUE)
legend("bottomright",legend = c("Over","Under"),lty=1, col=c("red","blue"))

```



