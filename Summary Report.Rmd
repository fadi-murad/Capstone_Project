---
title: "Income Classification Project - Summary Report"
author: "Fadi Murad"
date: "July 19, 2018"
output:
    html_document:
      theme: cerulean
      toc: true
      toc_depth: 4
      toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE,comment="")
```

<br />  

###1. Overview
This is a binary classification problem to predict income with fairly huge and imbalanced dataset (300,000 instances,41 attributes, and 94:6 class ratio). We will build different predictive models and compare their performance in order to choose the best model.


##### Problem Statement
**"Predict the income class of US population"**  

The aim for this project is to build a predictive model to determine the income level for people in US. The income levels are binned at below 50K and above 50K.

<br />  


##### Project Content
I divided this project into 4 sections, the first folder contains data sets while the rest represent the steps of this project and contain the related documents (in RMD and HTML formats) in addition to the supported files.  

Below are the 4 sections of our project where you can find the full details:  

1. Dataset
2. Data Exploration
3. Data Cleaning and Manipulation
4. Machine Learning

<br />  

###2. Data Set
Train set: 199,523 instances and 41 attributes, Test set: 99,762 instances and 41 attributes. You can find train and test sets in csv format with columns names and ready to be loaded.  

Weâ€™ve taken the data set from UCI Machine Learning Repository, and you can find the original data at:
http://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/

<br />  

###3. Data Exploration
This step includes loading data, prepare data and taking a look at each variable in order to understand the data.

```{r}
library(ggplot2)
library(caret)
load(file="2-Data_Exploration/tr.RData")
load(file="2-Data_Exploration/test.RData")

load(file="4-Machine_Learning/f_tr.RData")
load(file="4-Machine_Learning/f_test.RData")

```

```{r eval=FALSE}
load(file="tr.RData")
load(file="test.RData")

str(tr)
Hmisc::describe(tr)

str(test)
Hmisc::describe(test)

```

As we mentioned earlier we have 41 attributes, 7 attributes are continuous, while the rest are a string, most of the variables with string class needed to be modified.

```{r eval=FALSE}
unique(tr$income_level)
prop.table(table(tr$income_level)) #value distribution
summarise(group_by(tr,income_level),n=n()) #values count

```
**Target Variable "income_level":**
Class Ratio: **94:6**, the majority of instances fall into -50,000 class.  

As this is a binary classification problem, I will encode the income_level to 0 & 1 instead of -50000 & +50000 (please note there is a difference in the format of this column between train and test set).  

```{r eval=FALSE}
tr$income_level[tr$income_level==-50000] <- 0
tr$income_level[tr$income_level==+50000] <- 1

test$income_level[test$income_level=="-50000"] <- 0
test$income_level[test$income_level=="50000+."] <- 1

```


After checking data sets and target variable we will plot all variables in order to understand the distribution, levels, and relation with target variable, below 2 examples for age and wage per hour (you can find all the variables visualization in Data Exploration folder). 

```{r fig.height= 4 , fig.width=12}
gridExtra::grid.arrange(
  ggplot(tr,aes(x=age,y=..count.., fill = as.factor(income_level))) +
    geom_bar() + theme(panel.background = element_blank()) + labs(fill = "income level")+ 
    ggtitle("age vs. income level"),
  ggplot(tr,aes(x=age,y= wage_per_hour)) +
    geom_jitter(aes(col=as.factor(income_level),alpha = 0.5, shape = as.factor(income_level))) +
    theme(panel.background = element_blank())+ labs(fill = "income level")+
    scale_y_continuous("wage per hour", breaks = seq(0,10000,1000)) +
    ggtitle("Age vs. Wage per hour per Income Level"),
  ncol = 2
)

```


For the age variable, we can see that most of the people with income_level 1 fall into the age between *25* and *75* years, also we can notice the data has outliers at age 90
While for wage per hour, most of the people with income level 1 has a wage_per_hour between *1000* and *4000*.

<br />  

###4. Data Cleaning and Manipulation
For this step, we split each data set into 2 sets one for numeric variables and one for the categorical.

#####Missing Values
```{r}
catcol <- c(2:5,7,8:16,20:29,31:38,40)
numcol <- c(setdiff(1:40,catcol),41)

tr_num <- tr[numcol]
tr_cat <- tr[catcol]

test_num <- test[numcol]
test_cat <- test[catcol]
```

```{r}
trNumMissing <- sapply(tr_num, function(x){sum(is.na(x))/length(x)})
testNumMissing <- sapply(test_num, function(x){sum(is.na(x)/length(x))})

trCatMissing <- sapply(tr_cat, function(x){sum(is.na(x))/length(x)})
testCatMissing <- sapply(test_cat, function(x){sum(is.na(x)/length(x))})


trNumMissing

trCatMissing
```

When we calculate the missing values we will find that, there are no missing values in the numerical variables, while there are 9 variables have missing values, 3 of them have almost 50% missing values.
We will drop all the variables with more than 10% missing values from our set.

```{r eval=FALSE}
tr_cat <- subset(tr_cat, select = trCatMissing < 0.1 )
test_cat <- subset(test_cat, select = trCatMissing < 0.1)

```

<br />  

#####Near Zero Variance Variables

During the exploration step we saw a lot of variables have one level with very high frequency (around 90% of the data belong to one level) while the other levels have very low 
frequency ( between 1% to 4%), usually this kind of variables doesn't have much information
to offer to the predictive models, we will use nearZeroVar function to locate those variables.  

*nearZeroVar* diagnoses predictors that have one unique value or predictors that have 
both of the following characteristics: They have very few unique values relative to the 
number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large.  


**Numeric Variable**

```{r}
nzvNum <- nearZeroVar(tr_num, saveMetrics= TRUE)
subset(nzvNum, nzvNum$nzv == TRUE)
```

As we saw on the exploration step those variables have a huge amount of Zero values, we will reclassify those variable to categorical with only 2 levels "zero" & "more than zero". 

```{r eval=FALSE}
tr_num$wage_per_hour <- as.factor(ifelse(tr_num$wage_per_hour == 0,"Zero","MoreThanZero"))
tr_num$capital_gains <- as.factor(ifelse(tr_num$capital_gains == 0,"Zero","MoreThanZero"))
tr_num$capital_losses <- as.factor(ifelse(tr_num$capital_losses == 0,"Zero","MoreThanZero"))
tr_num$dividend_from_Stocks <- as.factor(ifelse(tr_num$dividend_from_Stocks == 0,"Zero","MoreThanZero"))

test_num$wage_per_hour <- as.factor(ifelse(test_num$wage_per_hour == 0,"Zero","MoreThanZero"))
test_num$capital_gains <- as.factor(ifelse(test_num$capital_gains == 0,"Zero","MoreThanZero"))
test_num$capital_losses <- as.factor(ifelse(test_num$capital_losses == 0,"Zero","MoreThanZero"))
test_num$dividend_from_Stocks <- as.factor(ifelse(test_num$dividend_from_Stocks == 0,"Zero","MoreThanZero"))

```

<br />  

**Categorical Variables**

We will drop all the variable where **nearZeroVar** is true.

```{r}
nzvCat <- nearZeroVar(tr_cat,freqCut = 90/10, saveMetrics= TRUE)
subset(nzvCat, nzvCat$nzv == TRUE)
```

```{r eval=FALSE}
tr_cat[nzvCat$nzv == TRUE] <- NULL
test_cat[nzvCat$nzv == TRUE] <- NULL
```

<br />  


### 5. Machine Learning

**Performance Measure:** In this project, we will use *Area Under the Curve (AUC)* to be the main performance measure for the below reasons:

1. Usually, AUC is a very good measure to use with imbalanced data.
2. We are trying to build a model that has a good prediction for both classes (0 and 1 or <50,000 and > 50,000). in other words, we don't have a preference or we don't value one class accuracy over the other and AUC can do that, unlike Sensitivity for example which focus on increasing the TP rate.  


**Sampling Techniques:** For imbalanced data, it's recommended to change the dataset to be more balanced in order to use it to build our predictive model.  

This change is called sampling dataset and there are a few techniques could be used, in this project, we will use over-sampling & under-sampling techniques.  

We will create 2 additional sets using the mentioned methods then we will apply the classification models on the original and the new sets and evaluate the results.  


**Algorithms:** We will use the below algorithms in this project:

1. Logistic Regression: we will start with a simple classification model. 
2. Random Forest: often perform well on imbalanced data. 
3. Naive Bayes: also a good algorithm for the imbalanced data.  

**Cross Validation Stategy:** 5 fold cross validation.

We have 2 very good packages for predictive modeling CARET and MLR, however, due to the limited performance of my machine, I will use MLR (train function in CARET package consume a high portion of memory during the runtime and create huge size objects).



```{r}
library(rJava)
library(mlr)
library(lattice)
library(ggplot2)
library(FSelector)
library(ROCR)

set.seed(50)

#create task for train and test sets
tr_Task <- makeClassifTask(data = f_tr,target = "income_level")
test_Task <- makeClassifTask(data = f_test,target = "income_level")

#Create sampling tasks in order to make the data balanced
#under-sampling 
tr_Under <- undersample(tr_Task,rate = 0.1) #keep only 10% of majority class

#over-sampling
tr_Over <- oversample(tr_Task,rate=15) #multiply minority class 15 times

#Set Validation Strategy: 5 fold cross validation
res_Mod <- makeResampleDesc("CV",iters=5 ,stratify = TRUE)

```

<br />  

##### Variable Importance  

Before starting with modeling, let us take a look at how important each variable to the target variable, we will use **generateFilterValuesData** function.  

We can notice that education and major occupation code could provide the highest info for our model while race variable has very litter to offer here.  



```{r}
variable_Imp <- generateFilterValuesData(tr_Task,method = "information.gain")
plotFilterValues(variable_Imp)

```


<br />  


***Now let us take a look at the result of each algorithm we applied in our project***  

<br />  

##### Logistic Regression Model

```{r eval=FALSE}
lr_learner <- makeLearner("classif.logreg",predict.type = "prob", fix.factors.prediction = TRUE)

```

```{r eval=FALSE}

lrCV_Task<- resample(lr_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
lrCV_Under<- resample(lr_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
lrCV_Over<- resample(lr_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
```

After running the resample function we got the below warning messages:  

*1: glm.fit: fitted probabilities numerically 0 or 1 occurred*  

*2: In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :prediction from a rank-deficient fit may be misleading*  

The 1st warning could be due to separation. 
There are many suggested solutions for this issue, do nothing and ignore the warning, drop the variables that cause this problem but in this case, we will lose information, and finally, utilize a form of penalized regression, in fact, this is the original reason some of the penalized regression forms were developed  

The 2nd warning could be due to excess of variables (this often happen in logistic regression when we have too many variables)

Anyhow, we will not go further into the logistic regression model, I just wanted to show the limitation of this model for our dataset. Let us check the naive bayes model, we may have better luck with it.

<br />  



##### Naive Bayes


```{r}
nb_learner <- makeLearner("classif.naiveBayes",predict.type = "prob", fix.factors.prediction = TRUE)
```


```{r cache= TRUE, results='hide',hide = TRUE}
nbCV_Task<- resample(nb_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
nbCV_Under<- resample(nb_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
nbCV_Over<- resample(nb_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))

```

Below we can find the performance measures after running the resample function on the 3 sets, All 3 models almost have the same AUC *0.92*, the original task has the best tpr while under and over tasks have the best tnr  
we will train all the tasks and check their performance on the test set  


```{r}
nb_Original<- nbCV_Task$aggr
nb_Under<- nbCV_Under$aggr
nb_Over<- nbCV_Over$aggr

remove(nbCV_Task,nbCV_Over,nbCV_Under)

cbind(as.data.frame(nb_Original),as.data.frame(nb_Under),as.data.frame(nb_Over))
```


<br />  


```{r cache= TRUE,results='hide'}
nb_Mod_Original <- train(nb_learner, tr_Task)
nb_Pre_Original <- predict(nb_Mod_Original,test_Task)

nb_Mod_Over <- train(nb_learner, tr_Over)
nb_Pre_Over <- predict(nb_Mod_Over,test_Task)

nb_Mod_Under <- train(nb_learner, tr_Under)
nb_Pre_Under <- predict(nb_Mod_Under,test_Task)

```



<br />  

```{r}
conf_Mat <- function(pre,con_title){
  aa<- as.data.frame(round(prop.table(table(f_test$income_level,pre$data$response))*100,2)) 

  plo_Con <- ggplot() +
    geom_tile(aes(x=Var1, y=Var2,fill=Freq),data=aa, color="white",size=0.1) +
    labs(x="Actual Class",y="Predicted Class")+
    geom_text(aes(x=Var1,y=Var2, label=Freq),data=aa, size=4, colour="white") +
    scale_fill_gradient(breaks=seq(from=0, to=80000, by=10000))+
    ggtitle(con_title)
  
  plo_Con
}

```

**Result on Test Set**  

*confusion matrix for all 3 tasks (Percentage)*

```{r fig.height= 3 , fig.width=12}

gridExtra::grid.arrange(conf_Mat(nb_Pre_Original,"Original"),
                        conf_Mat(nb_Pre_Over,"Over"),
                        conf_Mat(nb_Pre_Under,"Under"),
                        ncol = 3)

```


*ROC curve(fpr vs. tpr) and Precision vs. Recall*

```{r fig.height= 5 , fig.width=12}
nbROCR_Th <- generateThreshVsPerfData(
  list(Original =nb_Pre_Original, Over = nb_Pre_Over, Under = nb_Pre_Under),measures = list(tpr,fpr,ppv) )
gridExtra::grid.arrange(
  plotROCCurves(nbROCR_Th,measures = list(fpr,tpr),diagonal = FALSE) + ggtitle("AUC"),
  plotROCCurves(nbROCR_Th,measures = list(tpr,ppv),diagonal = FALSE) + ggtitle("Precision vs Recall"),
  ncol = 2)

```

<br />  

It seems original model is slightly better than other models (especially with tpr measure). We will consider the original model as the best one on the naive bayes  


```{r}
mlr::performance(nb_Pre_Original,list(auc,acc,tpr,tnr,fpr,mmce))

```


<br />  


##### Random Forest Model


```{r}
rf_learner <- makeLearner("classif.h2o.randomForest",predict.type = "prob")
```


```{r cache= TRUE, results='hide'}
rfCV_Task<- resample(rf_learner,tr_Task,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
rfCV_Under<- resample(rf_learner,tr_Under,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))
rfCV_Over<- resample(rf_learner,tr_Over,res_Mod,measures = list(auc,acc,tpr,tnr,fpr,mmce))

```

Below we can find the performance measures after running the resample function on the 3 sets, original task performs bad for tnr *0.37*, over-sampling has very high auc & tnr *0.99* while under-sampling has a good auc,tpr & tnr with *0.94*, *0.88* & *0.85* respectively  

We will train both under and over tasks to compare their performance on the test set as I suspect overfitting for the over-sampling task in one of the classes


```{r}
rf_Original<-rfCV_Task$aggr
rf_Under<-rfCV_Under$aggr
rf_Over<-rfCV_Over$aggr

remove(rfCV_Over,rfCV_Task,rfCV_Under)

cbind(as.data.frame(rf_Original),as.data.frame(rf_Under),as.data.frame(rf_Over))

```



<br />  



```{r cache= TRUE,results='hide'}

rf_Mod_Over <- train(rf_learner, tr_Over)
rf_Pre_Over <- predict(rf_Mod_Over,test_Task)

rf_Mod_Under <- train(rf_learner, tr_Under)
rf_Pre_Under <- predict(rf_Mod_Under,test_Task)
```

**Result on Test Set**  

*confusion matrix (Percentage)*

```{r fig.height= 5 , fig.width=12}

gridExtra::grid.arrange(conf_Mat(rf_Pre_Over,"Over"),
                        conf_Mat(rf_Pre_Under,"Under"),ncol = 2)

```

<br />  

*ROC curve(fpr vs. tpr) and Precision vs. Recall*

```{r fig.height= 5 , fig.width=12}
rfROCR_Th <- generateThreshVsPerfData(
  list(Over = rf_Pre_Over, Under = rf_Pre_Under), measures = list(tpr,fpr,ppv) )

gridExtra::grid.arrange(
  plotROCCurves(rfROCR_Th,measures = list(fpr,tpr),diagonal = FALSE)+ggtitle("AUC") ,
  plotROCCurves(rfROCR_Th,measures = list(tpr,ppv),diagonal = FALSE) + ggtitle("Precision vs Recall"),
  ncol = 2)

```

<br />  

Both models have almost same performance measures in the test set, but over-sampling have *0.99* AUC on the training set and *0.94* on the test set while under-sampling maintains his AUC performance on both sets *0.94* which indicate that under-sampling is the more reliable model.  


*Best forest model model performance measures*
```{r}
mlr::performance(rf_Pre_Under,list(auc,acc,tpr,tnr,fpr,mmce))

```

<br />  

##### Comparison

*Naive Bayes vs. Random Forest*

```{r fig.height= 5 , fig.width=12}
SuROCR_Th <- generateThreshVsPerfData(
  list(NB_Original = nb_Pre_Original, RF_Under = rf_Pre_Under), measures = list(tpr,fpr,ppv) )

gridExtra::grid.arrange(
  plotROCCurves(SuROCR_Th,measures = list(fpr,tpr),diagonal = FALSE)+ggtitle("AUC") ,
  plotROCCurves(SuROCR_Th,measures = list(tpr,ppv),diagonal = FALSE) + ggtitle("Precision vs Recall"),
  ncol = 2)

```

Random forest under-sampling model is the best model in this project with **0.94 auc** comparing to the naive bayes original model with *auc 0.92*, therefore, we can say that our best model in this project is the random forest under-sampling.


<br />  

##### Room for Improvement?
Actually, there are some steps that we can do to improve our model result but due to the limitation of my machine performance, I couldn't run those steps. Below you can find 2 examples to improve our random forest under-sampling model

**1. Feature Selection**  

We can use makeFeatSelControlRandom and selectFeatures functions to chose features that maximaze the auc measure as follow:

ctrl <- makeFeatSelControlRandom(maxit = 5)  

randF_SF <- selectFeatures(rf_learner,tr_Under,res_Mod,
                        measures = list(auc,acc,tpr,tnr),control = ctrl)


<br />  

**2. Model Tuning**  

Also, we can tune the parameters in the model using tuneParams function  

*Search for hyperparameters*  
rf_MP <- makeParamSet(makeIntegerParam("minsplit",lower = xx, upper = xx), makeIntegerParam("minbucket", lower = xx, upper = xx), 
makeNumericParam("cp", lower = xx, upper = xx))  

*do a grid search*  
rf_C <- makeTuneControlGrid()  

*hypertune the parameters*  

stune <- tuneParams(learner = rf_learner, resampling = res_Mod, task = tr_Under, par.set = rf_MP, control = rf_C, measures = auc)  




